在同一个模型下，对一张图片添加一些肉眼难以发现的噪点，在同一个模型下可能会得到不同的预测结果。
这次学习的就是故意添加噪点来攻击模型的对抗样本技术。
对样本的攻击分为无目标攻击和有目标攻击。无目标攻击添加杂质以后要求所得结果离真实结果越远越好，有目标攻击不仅要使结果远离真实结果，同时也要让其接近目标结果。
所以无目标攻击的损失函数为：
![]()
