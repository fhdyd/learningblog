在同一个模型下，对一张图片添加一些肉眼难以发现的噪点，可能会得到不同的预测结果。
故意添加噪点来攻击模型的对抗样本技术，对样本的攻击分为无目标攻击和有目标攻击。无目标攻击添加杂质以后要求所得结果离真实结果越远越好，有目标攻击不仅要使结果远离真实结果，同时也要让其接近目标结果。
所以无目标攻击的损失函数为：
![](https://github.com/fhdyd/learningblog/blob/gh-pages/images/QQ%E6%88%AA%E5%9B%BE20201017162331.png)

有目标攻击的损失函数为：
![](https://github.com/fhdyd/learningblog/blob/gh-pages/images/QQ%E6%88%AA%E5%9B%BE20201017162426.png)

但跟训练模型不一样的是，这次要调节的参数是输入的图片而不是神经网络的参数。不是损失函数越小越好，为了得到最小的损失函数而过度调整图片会产生明显的影响容易被人察觉，所以要对图片进行一定的限制，即：
